<p>[<em>My project for <a href="https://www.eacambridge.org/technical-alignment-curriculum">AGI Safety Fundamentals programme</a> (~Oct 2021). Code &amp; pictures below. Cross-posted <a href="https://www.lesswrong.com/posts/raRSW3e9iYMwkqjBX/cnn-feature-visualization-in-50-lines-of-code">here</a>.</em>]</p>
<p>To me, reading about <a href="https://distill.pub/2020/circuits/zoom-in/">Feature Visualization</a> felt like one of the most revealing insights about CNNs in the last years. Seeing the idea &quot;this node finds eyes, this node finds mouths, the combination detects faces&quot; (oversimplified) actually implemented by the CNN was a pleasant surprise, as in, it suggests we might actually understand how NNs work. There&#39;s more reading on the <a href="https://www.eacambridge.org/agi-week-6">programme website here</a>, I can highly recommend the articles by <a href="https://distill.pub/2020/circuits/">Chris Olah&#39;s group on distill</a>.</p>
<p>Seeing this I think many of us immediately want to try this, and play around with it. There is of course the OpenAI <a href="https://microscope.openai.com/models">Microscope</a> to look at results, and the <a href="https://github.com/tensorflow/lucid">Lucid</a> library, but I wanted to actually reproduce the idea myself without relying on a somewhat black box (big library / OpenAI Microscope).</p>
<p>Almost all tutorials I found however used Lucid, and this really cool write-up <a href="https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030">&quot;How to visualize convolutional features in 40 lines of code&quot;</a> unfortunately starts with <code>from fastai.conv_learner import *</code>. In retrospective I think I could understand this now, but I didn&#39;t, and finding out which parts were fastai functions and what they do was rather tricky. I also didn&#39;t manage to install the required (older) version of fastai.</p>
<p>So I decided to have a go myself, and, luckily, I found that &quot;DeepDream&quot; is based a very similar idea and I could adopt most code from <a href="https://www.tensorflow.org/tutorials/generative/deepdream">this notebook</a> from Google AI. This isn&#39;t actually too complicated, especially broken down to the bare minimum:</p>
<ol>
<li>A trained network whose features we want to visualize</li>
<li>A loop to maximize the activation of a targeted node</li>
<li>A few lines to make and show an image.</li>
</ol>
<p>The whole code runs in about a minute on my laptop (no GPU).</p>
<p>1) The first part is easy, we get the pre-trained network from tensorflow.</p>
<pre><code class="lang-python">import tensorflow as tf
base_model = tf<span class="hljs-selector-class">.keras</span><span class="hljs-selector-class">.applications</span><span class="hljs-selector-class">.VGG19</span>(include_top=False, weights=<span class="hljs-string">'imagenet'</span>)
</code></pre>
<p>2) The next part in the code (it&#39;s mostly comments really), see the comments marked with <code>#</code> for explanations:</p>
<pre><code class="lang-python">def maximize_activation(starting_img,\
                        <span class="hljs-attr">target_layer="mixed0",</span> <span class="hljs-attr">target_index=0,\</span>
                        <span class="hljs-attr">steps=10,</span> <span class="hljs-attr">step_size=0.1):</span>

    <span class="hljs-comment"># Take the network and cut it off at the layer we want to analyze,</span>
    <span class="hljs-comment"># i.e. we only need the part from the input to the target_layer.</span>
    <span class="hljs-attr">target</span> = [base_model.get_layer(target_layer).output]
    <span class="hljs-attr">part_model</span> = tf.keras.Model(<span class="hljs-attr">inputs=base_model.input,</span> <span class="hljs-attr">outputs=target)</span>

    <span class="hljs-comment"># The next part is the function to maximize the target layer/node by</span>
    <span class="hljs-comment"># adjusting the input, equivalent to the usual gradient descent but</span>
    <span class="hljs-comment"># gradient ascent. Run an optimization loop:</span>
    def gradient_ascent(img, steps, step_size):
        <span class="hljs-attr">loss</span> = tf.constant(<span class="hljs-number">0.0</span>)
        for n <span class="hljs-keyword">in</span> tf.range(steps):
            <span class="hljs-comment"># As in normal NN training, you want to record the computation</span>
            <span class="hljs-comment"># of the forward-pass (the part_model call below) to compute the</span>
            <span class="hljs-comment"># gradient afterwards. This is what tf.GradientTape does.</span>
            <span class="hljs-keyword">with</span> tf.GradientTape() as tape:
                tape.watch(img)
                <span class="hljs-comment"># Forward-pass (compute the activation given our image)</span>
                <span class="hljs-attr">activation</span> = part_model(tf.expand_dims(img, <span class="hljs-attr">axis=0))</span>
                <span class="hljs-comment"># The activation will be of shape (1,N,N,L) where N is related to</span>
                <span class="hljs-comment"># the resolution of the input image (assuming our target layer is</span>
                <span class="hljs-comment"># a convolutional filter), and L is the size of the layer. E.g. for a</span>
                <span class="hljs-comment"># 256x256 image in "block4_conv1" of VGG19, this will be</span>
                <span class="hljs-comment"># (1,32,32,512) -- we select one of the 512 nodes (index) and</span>
                <span class="hljs-comment"># average over the rest (you can average selectively to affect</span>
                <span class="hljs-comment"># only part of the image but there's not really a point):</span>
                <span class="hljs-attr">loss</span> = tf.math.reduce_mean(activation[:,:,:,target_index])

            <span class="hljs-comment"># Get the gradient, i.e. derivative of "loss" with respect to input</span>
            <span class="hljs-comment"># and normalize.</span>
            <span class="hljs-attr">gradients</span> = tape.gradient(loss, img)
            gradients /= tf.math.reduce_std(gradients)

            <span class="hljs-comment"># In the final step move the image in the direction of the gradient to</span>
            <span class="hljs-comment"># increate the "loss" (our targeted activation). Note that the sign here</span>
            <span class="hljs-comment"># is opposite to the typical gradient descent (our "loss" is the target </span>
            <span class="hljs-comment"># activation which we maximize, not something we minimize).</span>
            <span class="hljs-attr">img</span> = img + gradients*step_size
            <span class="hljs-attr">img</span> = tf.clip_by_value(img, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)
        return loss, img

    <span class="hljs-comment"># Preprocessing of the image (converts from [0..255] to [-1..1]</span>
    <span class="hljs-attr">img</span> = tf.keras.applications.inception_v3.preprocess_input(starting_img)
    <span class="hljs-attr">img</span> = tf.convert_to_tensor(img)
    <span class="hljs-comment"># Run the gradient ascent loop</span>
    loss, <span class="hljs-attr">img</span> = gradient_ascent(img, tf.constant(steps), tf.constant(step_size))
    <span class="hljs-comment"># Convert back to [0..255] and return the new image</span>
    <span class="hljs-attr">img</span> = tf.cast(<span class="hljs-number">255</span>*(img + <span class="hljs-number">1.0</span>)/<span class="hljs-number">2.0</span>, tf.uint8)
    return img
</code></pre>
<p>3) Finally apply this procedure to a random image:</p>
<pre><code><span class="hljs-built_in">import</span> numpy as np
<span class="hljs-built_in">import</span> matplotlib.pyplot as plt

<span class="hljs-attr">starting_img</span> = np.random.randint(<span class="hljs-attr">low=0,high=255,size=(300,300,3),</span> <span class="hljs-attr">dtype=np.uint8)</span>
<span class="hljs-attr">optimized_img</span> = maximize_activation(starting_img, <span class="hljs-attr">target_layer="block4_conv1",</span> <span class="hljs-attr">target_index=47,</span> <span class="hljs-attr">steps=10,</span> <span class="hljs-attr">step_size=0.1)</span>
plt.imshow(np.array(optimized_img))
</code></pre><p>And here we go!</p>
<p><img src="https://github.com/Stefan-Heimersheim/tensorflow-feature-extraction-tutorial/blob/main/images/img01.png?raw=true" alt="generated image"></p>
<p>Looks like features. Now let&#39;s try to reproduce one of the OpenAI microscope images, node 4 of layer block4_conv1 -- here is my version:</p>
<p><img src="https://github.com/Stefan-Heimersheim/tensorflow-feature-extraction-tutorial/blob/main/images/img02.png?raw=true" alt="OpenAI Microscope reproduction"></p>
<p>And the OpenAI Microscope image:</p>
<p><img src="https://oaiggoh.blob.core.windows.net/microscopeprod/2020-07-25/2020-07-25/vgg19_caffe/lucid.feature_vis/_feature_vis/alpha%3DFalse%26negative%3DFalse%26objective%3Dchannel%26op%3Dconv4_1%252Fconv4_1%253A0%26repeat%3D0%26start%3D0%26steps%3D4096%26stop%3D32/channel-4.png" alt="OpenAI Microscope original"></p>
<p>Not identical, but clearly the same feature in both visualizations!</p>
<p>Finally here is a run with InceptionV3, just for the pretty pictures, this time starting with a non-random (black) image. And an animation of the image after every iteration.</p>
<p><img src="https://github.com/Stefan-Heimersheim/tensorflow-feature-extraction-tutorial/raw/main/image.png" alt="final image">
<img src="https://github.com/Stefan-Heimersheim/tensorflow-feature-extraction-tutorial/raw/main/image.gif" alt="animation"></p>
<p>Note: There&#39;s an <em>optional</em> bit to improve the speed (by about a factor of 2 on my laptop), just add this decorator in front of the <code>gradient_ascent</code> function:</p>
<pre><code>    @tf.function(
        <span class="hljs-comment"># Decorator to increase the speed of the gradient_ascent function</span>
        <span class="hljs-attr">input_signature=(</span>
          tf.TensorSpec(<span class="hljs-attr">shape=[None,None,3],</span> <span class="hljs-attr">dtype=tf.float32),</span>
          tf.TensorSpec(<span class="hljs-attr">shape=[],</span> <span class="hljs-attr">dtype=tf.int32),</span>
          tf.TensorSpec(<span class="hljs-attr">shape=[],</span> <span class="hljs-attr">dtype=tf.float32),)</span>
    )
    def gradient_ascent(img, steps, step_size): ...
</code></pre><hr>
<p>This is basically how far I got in the time, the code can be found on my GitHub (<a href="https://github.com/Stefan-Heimersheim/tensorflow-feature-extraction-tutorial/blob/main/short_version.ipynb">link</a>). But I do plan to look at some more interpretability techniques (maybe something for transformers or RL?) or more general AGI Safety ideas in the future!</p>
<p>Feel free to post a comment or send me a message if you have any questions or anything really, happy to chat about these things!</p>
<p><em>I just want to thank the organizers of the AGI Safety Fundamentals programme again, for setting up the programme and all their support. I can highly recommend the programme, as well as the well-curated curriculum <a href="https://www.eacambridge.org/technical-alignment-curriculum">here</a> if you just want to read through it yourself.</em> </p>
