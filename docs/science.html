<!doctype html>
<html>
<head>
	<meta charset="utf-8"/>
	<title> Science | heimersheim.eu </title>
	<link rel="stylesheet" type="text/css" href="style.css">
	<link rel="shortcut icon" href="favicon.ico">
</head>
<div id="header">
	<a id="headername" href="/"> Stefan Heimersheim </a>
	<span class="hidden"> - </span>
	<span id="headermirrors"> Links: <a href="https://github.com/Stefan-Heimersheim">GitHub</a></span>
</div>
<hr class="hidden"/>
<div id="menu">
	<a href='./index.html' title='Home'>Home</a>
	<a href='./outreach.html' title='Outreach'>Outreach</a>
	<a href='./projects.html' title='Projects'>Projects</a>
	<a href='./science.html' title='Science'><b>Science</b></a>
	<a href='./contact.html' title='Contact'>Contact</a>
</div>
<hr class="hidden"/>
<div id="content">
	<div id="nav">
		<ul>
				<li><a href='#Essence of the sea_cucumber'>Essence of the sea_cucumber</a></li>
		<li><a href='#Feature Vizualization'>Feature vizualization</a></li>
		<li><a href='#Planck CMB Likelihoods'>Planck cmb likelihoods</a></li>
		</ul>
	</div>
<hr class="hidden"/>
<div id="main">
<article>
<a href='#Essence of the sea_cucumber'><h1 id='Essence of the sea_cucumber'>Essence of the sea_cucumber</h1></a>
<p><a href="https://github.com/Stefan-Heimersheim/sea_cucumber_essence">[GitHub link]</a></p>
<p>Experiments with CNN features and circuits. 
In short, why does this (maximized node <code>4</code> in the <code>block5_conv4</code> layer of <code>VGG19</code>)</p>

<p><img src="https://github.com/Stefan-Heimersheim/sea_cucumber_essence/blob/main/node4.png?raw=true" alt="node4" width=224 class="center"/></p>

<p>look like <code>sea_cucumber</code> to all ImageNet-trained CNNs?</p>

<p><img src="https://github.com/Stefan-Heimersheim/sea_cucumber_essence/blob/main/tSNE.png?raw=true" alt="tSNE" width=90% class="center"/></p>

<h2 id="context">Context</h2>

<p><a href="https://github.com/Stefan-Heimersheim/sea_cucumber_essence/blob/main/README.md#investigation">[Skip to Investigation]</a></p>

<p>Using my <a href="https://github.com/Stefan-Heimersheim/tensorflow-feature-extraction-tutorial/">feature extraction</a> script I analyzed 
node <code>4</code> in the <code>block5_conv4</code> layer of <code>VGG19</code>:</p>

<pre><code class="python language-python">import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from PIL import Image 
</code></pre>

<pre><code class="python language-python">base_model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
target_layer="block5_conv4"
target_index=4
steps=100
step_size=0.1
# Take the network and cut it off at the layer we want to analyze,
# i.e. we only need the part from the input to the target_layer.
target = [base_model.get_layer(target_layer).output]
part_model = tf.keras.Model(inputs=base_model.input, outputs=target)
</code></pre>

<pre><code class="python language-python"># The next part is the function to maximize the target layer/node by
# adjusting the input, equivalent to the usual gradient descent but
# gradient ascent. Run an optimization loop:
activation = None
@tf.function(
    # Decorator to increase the speed of the gradient_ascent function
    input_signature=(
      tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),
      tf.TensorSpec(shape=[], dtype=tf.int32),
      tf.TensorSpec(shape=[], dtype=tf.float32),)
)
def gradient_ascent(img, steps, step_size):
    loss = tf.constant(0.0)
    for n in tf.range(steps):
        # As in normal NN training, you want to record the computation
        # of the forward-pass (the part_model call below) to compute the
        # gradient afterwards. This is what tf.GradientTape does.
        with tf.GradientTape() as tape:
            tape.watch(img)
            # Forward-pass (compute the activation given our image)
            activation = part_model(tf.expand_dims(img, axis=0))
            print(activation)
            print(np.shape(activation))
            # The activation will be of shape (1,N,N,L) where N is related to
            # the resolution of the input image (assuming our target layer is
            # a convolutional filter), and L is the size of the layer. E.g. for a
            # 256x256 image in "block4_conv1" of VGG19, this will be
            # (1,32,32,512) -- we select one of the 512 nodes (index) and
            # average over the rest (you can average selectively to affect
            # only part of the image but there's not really a point):
            loss = tf.math.reduce_mean(activation[:,:,:,target_index])

        # Get the gradient, i.e. derivative of "loss" with respect to input
        # and normalize.
        gradients = tape.gradient(loss, img)
        gradients /= tf.math.reduce_std(gradients)

        # In the final step move the image in the direction of the gradient to
# increate the "loss" (our targeted activation). Note that the sign here
# is opposite to the typical gradient descent (our "loss" is the target 
# activation which we maximize, not something we minimize).
        img = img + gradients*step_size
        img = tf.clip_by_value(img, -1, 1)
    return loss, img
</code></pre>

<pre><code class="python language-python"># Preprocessing of the image (converts from [0..255] to [-1..1]
starting_img = np.random.randint(low=0,high=255,size=(224,224,3), dtype=np.uint8)
img = tf.keras.applications.vgg19.preprocess_input(starting_img)
img = tf.convert_to_tensor(img)
# Run the gradient ascent loop
loss, img = gradient_ascent(img, tf.constant(steps), tf.constant(step_size))
# Convert back to [0..255] and return the new image
img = tf.cast(255*(img + 1.0)/2.0, tf.uint8)
plt.imshow(np.array(img))
im = Image.fromarray(np.array(img))
im.save("node4.png")
</code></pre>

<h2 id="theconfusingpart">The confusing part</h2>

<p>Judging my the <a href="https://microscope.openai.com/models/vgg19_caffe/conv5_4_conv5_4_0/4">OpenAI Microscope</a> it looks like the node mostly gets activated by furry animals -- <em>in the training set</em>. Of course our image in artificial and this far outside the usual distribution, and we can expect such different behaviour. But why do we get the <code>sea_cucumber</code> prediction, rather than predictions of <code>dog</code>, <code>bison</code> or <code>lion</code>?</p>

<p>Feeding this image into the network, it seems insanely sure that the right label is <code>sea_cucumber</code>. Also other imagenet-trained networks such as Inception or VGG16 give the same result. Note: This was not indended and not optimized for.</p>

<pre><code class="python language-python">model_vgg19 = tf.keras.applications.VGG19(weights='imagenet', include_top=True)
x = tf.keras.applications.vgg19.preprocess_input(np.expand_dims(img, axis=0))
predictions = model_vgg19.predict(x)
print('Predicted:', tf.keras.applications.vgg19.decode_predictions(predictions, top=3)[0])
</code></pre>

<pre><code>Predicted: [('n02321529', 'sea_cucumber', 1.0), ('n01924916', 'flatworm', 1.2730256e-33), ('n01981276', 'king_crab', 2.537045e-37)]
</code></pre>

<pre><code class="python language-python">model_vgg16 = tf.keras.applications.VGG16(weights='imagenet', include_top=True)
x = tf.keras.applications.vgg16.preprocess_input(np.expand_dims(img, axis=0))
predictions = model_vgg16.predict(x)
print('Predicted:', tf.keras.applications.vgg16.decode_predictions(predictions, top=3)[0])
</code></pre>

<pre><code>Predicted: [('n02321529', 'sea_cucumber', 1.0), ('n01950731', 'sea_slug', 4.6657154e-15), ('n01924916', 'flatworm', 1.810621e-15)]
</code></pre>

<pre><code class="python language-python">model_resnet = tf.keras.applications.ResNet50(weights='imagenet', include_top=True)
x = tf.keras.applications.resnet.preprocess_input(np.expand_dims(img, axis=0))
predictions = model_resnet.predict(x)
print('Predicted:', tf.keras.applications.resnet.decode_predictions(predictions, top=3)[0])
</code></pre>

<pre><code>Predicted: [('n02321529', 'sea_cucumber', 0.9790509), ('n12144580', 'corn', 0.00899157), ('n13133613', 'ear', 0.005869923)]
</code></pre>

<p>Even this online service (<a href="https://www.snaplogic.com/machine-learning-showcase/image-recognition-inception-v3">snaplogic using Inception</a>) mistakes a picture of my phone screen showing the image:
<img src="https://github.com/Stefan-Heimersheim/sea_cucumber_essence/blob/main/recognize.png?raw=true" alt="recognize" width=80% class="center"/></p>

<h2 id="investigation">Investigation</h2>

<p>Let's look at the activations, after feeding the image into the VGG19 network I have been using:</p>

<pre><code class="python language-python">target = [model_vgg19.get_layer("block5_conv4").output]
model_vgg19_cutoff = tf.keras.Model(inputs=model_vgg19.input, outputs=target)
x = tf.keras.applications.vgg19.preprocess_input(np.expand_dims(img, axis=0))
activations = model_vgg19_cutoff.predict(x)
plt.plot(np.mean(np.mean(np.mean(activations, axis=0), axis=0), axis=0))
</code></pre>

<p><img src="https://github.com/Stefan-Heimersheim/sea_cucumber_essence/blob/main/activations.png?raw=true" width=100% class="center" />
So the question we're asking, is this the typical pattern for a dog or bison? Or maybe closer to the <code>sea_cucumber</code> pattern, in this 512-dimensional space?</p>

<p>Let's have a look at the <code>groenendael</code> (1st image in Microscope) and <code>sea_cucumber</code> classes, as well as a few randomly selected ones. I downloaded the imagenet data and used <a href="https://image-net.org/challenges/LSVRC/2017/browse-synsets.php">this list</a> to find the right files.
<img src="https://github.com/Stefan-Heimersheim/sea_cucumber_essence/blob/main/groenendael.png?raw=true" width=100% class="center" />
Hmm I don't really see a pattern by eye here, nor a similarity to above / excitation in index 4. In hindsight this makes sense, we wouldn't expect the category to be simply 1-hot encoded in activation space, because a) there is not enough room, and b) there are more layers following so I would rather think of some clusters in the high dimensional activation space. Let's maybe look some summary statistic, like the absolute distance in this 512-dim vector space.</p>

<p>So I take the training images, feed them into the network and read of the activations of the 512 nodes in the layer we are looking at (averaged over the 14x14 locations). Then I compute the distance as absolute distance between the vectors, 512-dimenisonal L2 norm.
The image below shows the distance between the optimized "sea_cucumber essence" image and the activations of <code>sea_cucumber</code> training data (green), <code>groenendael</code> (blue), and a mix of 10 random classes (100 random images each). The blue curve shows the average activation-distance between randomly selected images of different classes. The code for all the following plots can be found in <code>code_distances.py</code>.
<img src="https://github.com/Stefan-Heimersheim/sea_cucumber_essence/blob/main/activation_distances_node4maximized.png?raw=true" alt="distances" width=100% class="center"/></p>

<p>For context, here is the average distance between randomly selected images (grey), images from the same class (red) and images from different classes (blue):
<img src="https://github.com/Stefan-Heimersheim/sea_cucumber_essence/blob/main/activation_distances_general.png?raw=true" width=60% class="center" />
We learn three main things here:</p>

<ol>
<li>Generally images of the same class seem to be nearer to each other in this 512-dim space than random / different classes, but the effect is not very strong. Of course we wouldn't expect that the distance is the best measure of "closeness" between activations.</li>

<li>These numbers are all waaaay smaller than the ~7k and 36k we get from the "sea_cucumber essence" image. This tells us (somewhat unsurprisingly) that that optimized image is far outside the training distribution in at least this measure.</li>

<li>The <code>sea_cucumber</code> training data seems to give activations <em>slightly</em> closer to the "sea_cucumber essence" image -- so maybe it's just far outside the distribution but into the <code>sea_cucumber</code> direction?</li>
</ol>

<p>Naturally the L2-distance isn't the ideal way to reduce the 512-d space into something plot-able. One method I found is <a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html">t-SNE</a> which projects the 512-dimensions into two parameters which we can plot:
<img src="https://github.com/Stefan-Heimersheim/sea_cucumber_essence/blob/main/tSNE.png?raw=true" width=100% class="center" />
Looks like we get a nice separation (t-SNE does not know the labels) of different categories, and the "sea_cucumber essence" activations tend to lie within the <code>sea_cucumber</code> training data!</p>

<p>This doesn't definitely answer the question, but I think it's clear that this node4-maximized image ends up in a corner of parameter space which, even though it is "far away" (L2 distance), lies in a region that is clearly near the region that <code>sea_cucumber</code> training images lie in. Presented with this out-of-distribution image, and tasked with choosing between only the existing categories, the network decides for <code>sea_cucumber</code>.</p>
<a href='#Feature Vizualization'><h1 id='Feature Vizualization'>Feature vizualization</h1></a>
<p>[<em>My project for <a href="https://www.eacambridge.org/technical-alignment-curriculum">AGI Safety Fundamentals programme</a> (~Oct 2021). Code &amp; pictures below. Cross-posted on <a href="https://www.lesswrong.com/posts/raRSW3e9iYMwkqjBX/cnn-feature-visualization-in-50-lines-of-code">LessWrong</a>.</em>]</p>
<p>To me, reading about <a href="https://distill.pub/2020/circuits/zoom-in/">Feature Visualization</a> felt like one of the most revealing insights about CNNs in the last years. Seeing the idea &quot;this node finds eyes, this node finds mouths, the combination detects faces&quot; (oversimplified) actually implemented by the CNN was a pleasant surprise, as in, it suggests we might actually understand how NNs work. There&#39;s more reading on the <a href="https://www.eacambridge.org/agi-week-6">programme website here</a>, I can highly recommend the articles by <a href="https://distill.pub/2020/circuits/">Chris Olah&#39;s group on distill</a>.</p>
<p>Seeing this I think many of us immediately want to try this, and play around with it. There is of course the OpenAI <a href="https://microscope.openai.com/models">Microscope</a> to look at results, and the <a href="https://github.com/tensorflow/lucid">Lucid</a> library, but I wanted to actually reproduce the idea myself without relying on a somewhat black box (big library / OpenAI Microscope).</p>
<p>Almost all tutorials I found however used Lucid, and this really cool write-up <a href="https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030">&quot;How to visualize convolutional features in 40 lines of code&quot;</a> unfortunately starts with <code>from fastai.conv_learner import *</code>. In retrospective I think I could understand this now, but I didn&#39;t, and finding out which parts were fastai functions and what they do was rather tricky. I also didn&#39;t manage to install the required (older) version of fastai.</p>
<p>So I decided to have a go myself, and, luckily, I found that &quot;DeepDream&quot; is based a very similar idea and I could adopt most code from <a href="https://www.tensorflow.org/tutorials/generative/deepdream">this notebook</a> from Google AI. This isn&#39;t actually too complicated, especially broken down to the bare minimum:</p>
<ol>
<li>A trained network whose features we want to visualize</li>
<li>A loop to maximize the activation of a targeted node</li>
<li>A few lines to make and show an image.</li>
</ol>
<p>The whole code runs in about a minute on my laptop (no GPU).</p>
<p>1) The first part is easy, we get the pre-trained network from tensorflow.</p>
<pre><code class="lang-python">import tensorflow as tf
base_model = tf<span class="hljs-selector-class">.keras</span><span class="hljs-selector-class">.applications</span><span class="hljs-selector-class">.VGG19</span>(include_top=False, weights=<span class="hljs-string">'imagenet'</span>)
</code></pre>
<p>2) The next part in the code (it&#39;s mostly comments really), see the comments marked with <code>#</code> for explanations:</p>
<pre><code class="lang-python">def maximize_activation(starting_img,\
                        <span class="hljs-attr">target_layer="mixed0",</span> <span class="hljs-attr">target_index=0,\</span>
                        <span class="hljs-attr">steps=10,</span> <span class="hljs-attr">step_size=0.1):</span>

    <span class="hljs-comment"># Take the network and cut it off at the layer we want to analyze,</span>
    <span class="hljs-comment"># i.e. we only need the part from the input to the target_layer.</span>
    <span class="hljs-attr">target</span> = [base_model.get_layer(target_layer).output]
    <span class="hljs-attr">part_model</span> = tf.keras.Model(<span class="hljs-attr">inputs=base_model.input,</span> <span class="hljs-attr">outputs=target)</span>

    <span class="hljs-comment"># The next part is the function to maximize the target layer/node by</span>
    <span class="hljs-comment"># adjusting the input, equivalent to the usual gradient descent but</span>
    <span class="hljs-comment"># gradient ascent. Run an optimization loop:</span>
    def gradient_ascent(img, steps, step_size):
        <span class="hljs-attr">loss</span> = tf.constant(<span class="hljs-number">0.0</span>)
        for n <span class="hljs-keyword">in</span> tf.range(steps):
            <span class="hljs-comment"># As in normal NN training, you want to record the computation</span>
            <span class="hljs-comment"># of the forward-pass (the part_model call below) to compute the</span>
            <span class="hljs-comment"># gradient afterwards. This is what tf.GradientTape does.</span>
            <span class="hljs-keyword">with</span> tf.GradientTape() as tape:
                tape.watch(img)
                <span class="hljs-comment"># Forward-pass (compute the activation given our image)</span>
                <span class="hljs-attr">activation</span> = part_model(tf.expand_dims(img, <span class="hljs-attr">axis=0))</span>
                <span class="hljs-comment"># The activation will be of shape (1,N,N,L) where N is related to</span>
                <span class="hljs-comment"># the resolution of the input image (assuming our target layer is</span>
                <span class="hljs-comment"># a convolutional filter), and L is the size of the layer. E.g. for a</span>
                <span class="hljs-comment"># 256x256 image in "block4_conv1" of VGG19, this will be</span>
                <span class="hljs-comment"># (1,32,32,512) -- we select one of the 512 nodes (index) and</span>
                <span class="hljs-comment"># average over the rest (you can average selectively to affect</span>
                <span class="hljs-comment"># only part of the image but there's not really a point):</span>
                <span class="hljs-attr">loss</span> = tf.math.reduce_mean(activation[:,:,:,target_index])

            <span class="hljs-comment"># Get the gradient, i.e. derivative of "loss" with respect to input</span>
            <span class="hljs-comment"># and normalize.</span>
            <span class="hljs-attr">gradients</span> = tape.gradient(loss, img)
            gradients /= tf.math.reduce_std(gradients)

            <span class="hljs-comment"># In the final step move the image in the direction of the gradient to</span>
            <span class="hljs-comment"># increate the "loss" (our targeted activation). Note that the sign here</span>
            <span class="hljs-comment"># is opposite to the typical gradient descent (our "loss" is the target </span>
            <span class="hljs-comment"># activation which we maximize, not something we minimize).</span>
            <span class="hljs-attr">img</span> = img + gradients*step_size
            <span class="hljs-attr">img</span> = tf.clip_by_value(img, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)
        return loss, img

    <span class="hljs-comment"># Preprocessing of the image (converts from [0..255] to [-1..1]</span>
    <span class="hljs-attr">img</span> = tf.keras.applications.inception_v3.preprocess_input(starting_img)
    <span class="hljs-attr">img</span> = tf.convert_to_tensor(img)
    <span class="hljs-comment"># Run the gradient ascent loop</span>
    loss, <span class="hljs-attr">img</span> = gradient_ascent(img, tf.constant(steps), tf.constant(step_size))
    <span class="hljs-comment"># Convert back to [0..255] and return the new image</span>
    <span class="hljs-attr">img</span> = tf.cast(<span class="hljs-number">255</span>*(img + <span class="hljs-number">1.0</span>)/<span class="hljs-number">2.0</span>, tf.uint8)
    return img
</code></pre>
<p>3) Finally apply this procedure to a random image:</p>
<pre><code><span class="hljs-built_in">import</span> numpy as np
<span class="hljs-built_in">import</span> matplotlib.pyplot as plt

<span class="hljs-attr">starting_img</span> = np.random.randint(<span class="hljs-attr">low=0,high=255,size=(300,300,3),</span> <span class="hljs-attr">dtype=np.uint8)</span>
<span class="hljs-attr">optimized_img</span> = maximize_activation(starting_img, <span class="hljs-attr">target_layer="block4_conv1",</span> <span class="hljs-attr">target_index=47,</span> <span class="hljs-attr">steps=10,</span> <span class="hljs-attr">step_size=0.1)</span>
plt.imshow(np.array(optimized_img))
</code></pre><p>And here we go!</p>
<p><img src="https://github.com/Stefan-Heimersheim/tensorflow-feature-extraction-tutorial/blob/main/images/img01.png?raw=true" alt="generated image"></p>
<p>Looks like features. Now let&#39;s try to reproduce one of the OpenAI microscope images, node 4 of layer block4_conv1 -- here is my version:</p>
<p><img src="https://github.com/Stefan-Heimersheim/tensorflow-feature-extraction-tutorial/blob/main/images/img02.png?raw=true" alt="OpenAI Microscope reproduction"></p>
<p>And the OpenAI Microscope image:</p>
<p><img src="https://oaiggoh.blob.core.windows.net/microscopeprod/2020-07-25/2020-07-25/vgg19_caffe/lucid.feature_vis/_feature_vis/alpha%3DFalse%26negative%3DFalse%26objective%3Dchannel%26op%3Dconv4_1%252Fconv4_1%253A0%26repeat%3D0%26start%3D0%26steps%3D4096%26stop%3D32/channel-4.png" alt="OpenAI Microscope original"></p>
<p>Not identical, but clearly the same feature in both visualizations!</p>
<p>Finally here is a run with InceptionV3, just for the pretty pictures, this time starting with a non-random (black) image. And an animation of the image after every iteration.</p>
<p><img src="https://github.com/Stefan-Heimersheim/tensorflow-feature-extraction-tutorial/raw/main/image.png" alt="final image">
<img src="https://github.com/Stefan-Heimersheim/tensorflow-feature-extraction-tutorial/raw/main/image.gif" alt="animation"></p>
<p>Note: There&#39;s an <em>optional</em> bit to improve the speed (by about a factor of 2 on my laptop), just add this decorator in front of the <code>gradient_ascent</code> function:</p>
<pre><code>    @tf.function(
        <span class="hljs-comment"># Decorator to increase the speed of the gradient_ascent function</span>
        <span class="hljs-attr">input_signature=(</span>
          tf.TensorSpec(<span class="hljs-attr">shape=[None,None,3],</span> <span class="hljs-attr">dtype=tf.float32),</span>
          tf.TensorSpec(<span class="hljs-attr">shape=[],</span> <span class="hljs-attr">dtype=tf.int32),</span>
          tf.TensorSpec(<span class="hljs-attr">shape=[],</span> <span class="hljs-attr">dtype=tf.float32),)</span>
    )
    def gradient_ascent(img, steps, step_size): ...
</code></pre><hr>
<p>This is basically how far I got in the time, the code can be found on my GitHub (<a href="https://github.com/Stefan-Heimersheim/tensorflow-feature-extraction-tutorial/blob/main/short_version.ipynb">link</a>). But I do plan to look at some more interpretability techniques (maybe something for transformers or RL?) or more general AGI Safety ideas in the future!</p>
<p>Feel free to post a comment or send me a message if you have any questions or anything really, happy to chat about these things!</p>
<p><em>I just want to thank the organizers of the AGI Safety Fundamentals programme again, for setting up the programme and all their support. I can highly recommend the programme, as well as the well-curated curriculum <a href="https://www.eacambridge.org/technical-alignment-curriculum">here</a> if you just want to read through it yourself.</em> </p>

<a href='#Planck CMB Likelihoods'><h1 id='Planck CMB Likelihoods'>Planck cmb likelihoods</h1></a>
<p>The <a href="https://en.wikipedia.org/wiki/Planck_(spacecraft)">Planck spacecraft</a> is a satellite built to measure the Cosmic Microwave Background, the microwave radiation reaching us from the edge of the observable universe. Its goal is to detect tiny fluctuations (on the scale of 1 part in 100,000) of this radiation from different directions.
<br>
We use these observations to compare out theoretical models to reality. But because those fluctuations are based on randomness (quantum fluctuations in the early universe) we cannot predict the actual pattern on the sky. What we can predict, and test, are certain statistical properties. Most commonly we use the <i>multipoles</i> (usually referred to as the power spectrum) -- what are those? They describe how correlated or different the radiation in directions looks, e.g. <i>dipole</i> relating to the correlation of opposite directions. Another way to think of this is the correlation of two directions at a certain (angular) distance. The dipole describes correlation of points 180 degrees apart, quadrupole 90 degrees etc. The 180th moment describes points separated by an angle of just 1 degree and this turns out to be the "strongest" correlation. In simple terms, we could say the circumstances in the early universe allowed matter and energy to travel such that spots about 40 million light years apart were very correlated.
</p>

<p>For my work I frequently compute this power spectrum and compare it to the observations of Planck. For most of my work I used the code <a href="https://github.com/CobayaSampler/cobaya">cobaya</a> which automatically performs the calculation internally and returns the likelihood of a set of parameters being compatible with the Planck observations. One day I ran a model which was not included in cobaya, so I had to compute the likelihood myself and found surprisingly little documentation on the likelihood.
<br>
I did some <s>reverse-engineering</s>reading of the <a href="https://github.com/marius311/cosmoslik">cosmoslik wrapper</a> to find out how it works, and want to write about the results here. But before I continue, if you just want to get the likelihood of a given power spectrum use cosmoslik's <code>cosmoslik.likelihoods</code> module! Simply pass a dictionary of power spectra and you get the corresponding loglikelihood value.
</p>
<p>Just out of curiosity though, I wanted to figure out how exactly I can use the Planck likelihoods without any external programs, also to make sure that the wrapper was still functioning. <a href="https://github.com/marius311/cosmoslik/blob/master/cosmoslik_plugins/likelihoods/planck/clik.py">cosmoslik's code</a> turned out to be very helpful and largely what this is based on. So here's my writeup of how to use the clik python code (this is still a wrapper for the C &amp; Fortran code but since it comes together with the likelihood code from the Planck team I won't dig deeper here). Firstly, assuming you have installed the likelihood and sourced the <code>clik_profile</code> script, you can import the clik python module and load a clik likelihood file:</p>
<pre><code>from clik import clik
lowT = clik(&quot;/path/to/baseline/plc_3.0/low_l/commander/commander_dx12_v3_2_29.clik&quot;)
</code></pre>
<p>Now the function <code>lowT</code> can be called with a list as argument and returns the likelihood. To figure out which values to put in the list, we use the function <code>lowT.get_lmax()</code>. The former returns <code>(29, -1, -1, -1, -1, -1)</code> indicating it requires the TT spectra from l=0 to 29. The list maps to <code>(TT, EE, BB, TE, ?, ?)</code> where the latter two should be <code>TB</code> and <code>EB</code> but I'm not sure about the order.
Next use <code>lowT.get_extra_parameter_names()</code> to get the nuisance parameters that need to be appended to the list, in this case <code>('A_planck',)</code>.</p>
<p>First let us get the power spectra, e.g. from <a href="https://github.com/lesgourg/class_public">CLASS</a></p>
<pre><code>from classy import Class
cosmo = Class()
cosmo.set({'output': 'tCl pCl lCl', 'lensing': 'yes'})
cosmo.compute()
Cell = cosmo.lensed_cl()
</code></pre>
<p>Note that CLASS returns the power spectra in relative units while the likelihoods require absolute units (see Julien Lesgourgues's comment <a href="https://github.com/lesgourg/class_public/issues/322#issuecomment-613941965">here</a> for some context), so we convert to uK² by multiplying by the mean CMB temperature squared.</p>
<pre><code>Tcmb = 2.7255*1e6 #uK
Cl_TT = Cell['tt']*Tcmb**2
</code></pre>
<p>Also note that we just use the power spectrum C_ell, not the modified version D_ell. The latter is used as input for <code>cosmoslik</code>.</p>
<p>Finally we can call the likelihood with a list of spectra and nuisance parameters:</p>
<pre><code>A_planck = 1.000442
lowT([*Cl_TT[:30], A_planck])
</code></pre>
<p>For something close to bestfit LCDM the result should be around -12.</p>
<p>For a more complex example, let's look at the high-l TTTEEE likelihood. <code>highTTTEEE.lmax</code> shows us we need (2508, 2508, -1, 2508, -1, -1), i.e. the TT, EE and TE spectra, and in that order. <code>highTTTEEE.extra_parameter_names</code> lists a whopping 47 nuisance parameters which we have to pass in the right order as well.
Tipp: You can get those in the right order from a dictionary wit something like <code>nuisance_TTTEEE = [planck_nuisance_array[p] for p in highTTTEEE.extra_parameter_names]</code>.
Finally <code>highTTTEEE(Cl_TT[:2509]+Cl_EE[:2509]+Cl_TE[:2509]+nuisance_TTTEEE)</code> should return something like -1173, again for LCDM.</p>

<footer>
  <hr>
  <p>
  	<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons Licence" style="border-width:0; float:left; padding-right:0.5em;" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" />
  		</a><span>All original content on this website is, unless otherwise noted, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</span>
  </p>
</footer>
</article>
</div>
</div>
</html>